{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "MIS 382N - HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit ('mis-382n': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "4361aab12e46f73717a057315632a39c5720c41841579418b91b07fe756a6691"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <p style=\"text-align: center;\">MIS 382N: Advanced Machine Learning</p>\n",
        "# <p style=\"text-align: center;\">Homework 1</p>\n",
        "## <p style=\"text-align: center;\">Total points: 55</p>\n",
        "## <p style=\"text-align: center;\">Due: Wednesday, Sep 8 submitted via Canvas by 11:59 pm</p>\n",
        "\n",
        "Your homework should be written in a **Jupyter notebook**. Please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
        "\n",
        "**Note: Notebooks MUST have the images embedded in them. There will be no regrades if attached images do not render in the notebook. Please re download from canvas after submission and make sure all attached images render without errors. (Hint: Image module from IPython.display)**\n",
        "\n",
        "***\n",
        "\n",
        "# Team Members\n",
        "\n",
        "- ## Rohitashwa Chakraborty _(rc47878)_\n",
        "- ## Sahitya Sundar Raj Vijayanagar _(sv25849)_\n"
      ],
      "metadata": {
        "id": "evHVxwk2JA-3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: MLOps (10 pts)\n",
        "Read this [article](https://towardsdatascience.com/what-is-mlops-everything-you-must-know-to-get-started-523f2d0b8bd8) \"What is MLOps — Everything You Must Know to Get Started\", which gives a quick walkthrough of the machine learning development lifecycle and explains how MLOps come into play, or watch this [video](https://www.youtube.com/watch?v=06-AZXmwHjo) which you may find interesting.\n",
        "\n",
        "## 1. (**4 pts**) Use your own words to describe what MLOps is, and what challenges MLOps address. Limit your answer to one paragraph\n",
        "\n",
        "### **Answer 1**\n",
        "\n",
        "MLOps is an engineering discipline that focuses on the _unification_, _standardisation_ and _streamlining_ of Machine Learning or Deep Learning systems. The primary aim is to facilitate continuous development and deployment of high-performing models in production environments _(i.e: MLOps is the intersection of Data Science and DevOps)_. As Machine Learning solutions become increasingly ubiqutous, so do the technical challenges of maintaining these systems. A typical ML solution lifecycle requires several cross-functional teams to work in tandem:\n",
        "\n",
        "- **Business development or Product team** — _defining business objective with KPIs_\n",
        "- **Data Engineering** — _data acquisition and preparation_\n",
        "- **Data Science** — _architecting ML solutions and developing models_\n",
        "- **IT / DevOps** — _complete deployment setup, monitoring alongside scientists_\n",
        "\n",
        "Major challenges and bottlenecks that MLOps seeks to resolve are:\n",
        "\n",
        "- Shortage of Data Scientists who are good at developing and deploying scalable web applications\n",
        "- Reflect changing business objectives in the model\n",
        "- Communicate between the Technical and Functional teams\n",
        "- Risk assessment\n",
        "\n",
        "## 2. (**6 pts**) Describe what the main phases in MLOps are. Your answer should be 2-3 paragraphs\n",
        "\n",
        "### **Answer 2**\n",
        "\n",
        "The MLOps cycle has been divided into 7 key phases. To begin with, the **problem statement is formulated** by the business experts and Product Managers based on a business objective. After defining the objective, we translate it to an ML problem. Next, the ML model relies on **finding data** from credible sources, check compliances, frequency of change in data i.e. static or real-time data, identifying cloud services, and deciding on the data pipeline that can be used for the model training and optimization after deployment.\n",
        "\n",
        "Following the previous steps, the data engineering team works on **data extraction** from various sources and **data preparation** steps like feature engineering (identifying features that would go into the model), and data cleaning. In order to make the process more structured and feed clean data to the next step of the model development, the **data pipeline** is designed and coded. This would require a careful selection of cloud services and cloud architectures that are compatible with each other and fit well with the data requirement. With this, the next stage, i.e, **Model Building**, wherein data scientists use the cleaned data to train models iteratively and implement quantitative checks like error, accuracy, precision, recall, etc. and qualitative measures like R-square to narrow down the model that performs better. Other tasks at this stage would be using versioning for reproducibility, check performance with baseline models, and model scaling across distributed systems.\n",
        "\n",
        "**Building and automating pipelines** based on system requirements is integral. This would mean selection of appropriate cloud solutions, building and training the pipelines, auditing the pipelines, and validating the data. Once this stage has been completed, the pipeline is ready to be deployed, and this can be done in two ways. In **static deployment**, the model is packaged into an application software and deployed, whereas, in **dynamic deployment**, a web framework like FastAPI is used to deploy the pipeline and the user requests are sent to an API endpoint for servicing. Finally, it is crucial that organizations keep the model performance in check and **monitor** it to ensure that the model meets governance standards of all stakeholders. Here, both DevOps and Data Scientists are required to keep track and **maintain** the model, run logging to track issues, troubleshoot any failures, and **optimize models** for better performance.\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "TWWU_gOSLEEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Applications of Machine Learning (5 pts)\n",
        "\n",
        "Read this [article](https://builtin.com/data-science/data-science-applications-examples) \"17 Data Science Applications & Examples\" and pick one of the data science systems used by various organizations according to this blog.\n",
        "\n",
        "For this system you have chosen, answer the following questions. Please limit your answer to one paragraph:\n",
        "\n",
        "## 1. What kind of machine learning problem is involved (e.g. classification, regression, clustering, outlier detection,...) in this system?\n",
        "\n",
        "The UberEats delivery service works on a machine learning problem to predict how different factors would impact delivery times to their customers. This would be a **_regression_** algorithm where the target variable would be delivery times and the independent variables could be factors like traffic, restaurant location, delivery location, time of order, etc. Apart from just predicting delivery times, UberEats, being a food delivery service, also uses **_recommender systems_** to recommend cuisines and restaurants to their users. Another way in which machine learning can be used in UberEats would be to **_cluster_** users into different classes based on their usage for marketing and promotional purposes.\n",
        "\n",
        "## 2. Speculate on what kind of data may be needed and how the results can be useful to the organization\n",
        "\n",
        "In general, data about **_customers_** (name, age, gender, location, frequency of ordering, customer rating, favorite cuisine, most frequent restaurant, etc.), **_restaurants_**(location, cuisine, rating, menu, pricing), and **_delivery agents_**(name, rating, orders fulfilled) would be useful. For the purpose of estimating delivery times, **_User location details_**, **_Delivery agent location details_**, ***restaurant location details***, ***Traffic data***, ***Order timestamp*** would be helpful. Apart from just estimating delivery times, this would also enable the model to identify best routes between source and destination. In order for marketing and promotional purposes, customer data like ***favorite cuisine***, ***restaurant***, ***customer rating***, ***age***, ***frequency of ordering***, etc. would be most useful. Based on this information, UberEats can decide on how best to promote the product to a customer. Finally, for recommendation systems, order history details need to be tracked. This essentially means tracking details of every order (restaurant, cuisine, order date, order timestamp, delivery rating). Using feedback data for orders, sentiment analysis can be done to tune the rating of a restaurant.\n",
        "\n",
        "## 3. What do you think are the ethical implications of using machine learning in a domain like this?\n",
        "\n",
        "While using UberEats is the more convenient option for customers, ethically, there are a couple of concerns associated with the service. In the past, traditional restaurants, that benefited from having customers physically eat at the restaurant profited from their visit, wherein the entire revenue from the order went to the restaurant. However, with the popularity of UberEats, around 30% of the revenue goes to UberEats, that puts the restaurants and all its employees at a disadvantage. In the fear of losing out on customers in a competing market, restaurants do not have a choice but to go ahead and use these services. While ML models work on making things optimal, it does not take into consideration, the issues faced by drivers. Drivers tend to be incentivised less as the model would predict that they would drive long hours regardless, which affects the happiness quotient of drivers. Additionally, in order to meet the weekly target, drivers may tend to work overtime, affecting their health. Adding to all of this is the low pay that they receive. ML algorithms do not take into consideration underlying costs faced by drivers and does not include their feedback in designing algorithms. Hence, it is important to consider all stakeholders while designing the algorithm.\n",
        "***\n"
      ],
      "metadata": {
        "id": "wusA9L1LmUMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: Simpson's Paradox (10 pts)\n",
        "\n",
        "A data scientist should be careful about drawing unwarranted conclusions about any data that is presented. One of the 'gotchas' that can happen even in apparently very simple tabular summaries, is called Simpson's paradox.\n",
        "\n",
        "Read this [article](https://www.covid-datascience.com/post/israeli-data-how-can-efficacy-vs-severe-disease-be-strong-when-60-of-hospitalized-are-vaccinated), which explains why the computed efficacy of the Pfizer vaccine is misleadingly low (67.5%) when you lump all people together, but once you stratify people by age (which is the right thing to do), you get much higher efficacy numbers.\n",
        "\n",
        "## 1.(**5 pts**) Explain in your own words what Simpson's paradox is, and how this 'paradox' can happen in real data\n",
        "\n",
        "## 2.(**5 pts**) Find and mention another example of Simpson's paradox (but not any of the 3 examples given in the Wikipedia entry for 'Simpson's paradox'), state why the paradox appeared in your chosen example. Also give a reference (URL) to your source for the chosen example\n",
        "\n",
        "### References\n",
        "\n",
        "#### [California State Department of Developmental Services - 2014 Funding Allocation Dataset](https://www.kaggle.com/wduckett/californiaddsexpenditures)\n",
        "\n",
        "#### [Data Analysis Notebook](https://www.kaggle.com/captnemo/simpsons-paradox)\n"
      ],
      "metadata": {
        "id": "2w8za9lLmVO7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from IPython import display\n",
        "display.Image(\"https://raw.githubusercontent.com/rohitashwachaks/MIS-382N-Adv.-Machine-Learning/main/assignment/outputs/expenditure-by-ethnicity.png\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ethnicity Level Analysis\n",
        "\n",
        "Here we will be analysing the Simpson's paradox in the California State DSS spending for the year 2014.\n",
        "Upon aggregating the department's spending at an ethnicity level, we observe there exists a **huge disparity** between **_White not Hispanic_** and **_Hispanic_** communities.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis by Age Cohort\n",
        "\n",
        "However, if we divide the dataset into smaller bins, by age, we come across a startling revelation.\n",
        "\n",
        "It is observered that when observed by age, there is infact **no disparity** between ethnicity. This is an example of Simpson's Paradox, wherein, a trend observed at the higher level disappears, or even reverses, when observed at a granular level.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from IPython import display\n",
        "display.Image(\"https://raw.githubusercontent.com/rohitashwachaks/MIS-382N-Adv.-Machine-Learning/main/assignment/outputs/expenditure-0to5.png\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "display.Image(\"https://raw.githubusercontent.com/rohitashwachaks/MIS-382N-Adv.-Machine-Learning/main/assignment/outputs/expenditure-13to17.png\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "display.Image(\"https://raw.githubusercontent.com/rohitashwachaks/MIS-382N-Adv.-Machine-Learning/main/assignment/outputs/expenditure-18to21.png\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "display.Image(\"https://raw.githubusercontent.com/rohitashwachaks/MIS-382N-Adv.-Machine-Learning/main/assignment/outputs/expenditure-22to50.png\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "display.Image(\"https://raw.githubusercontent.com/rohitashwachaks/MIS-382N-Adv.-Machine-Learning/main/assignment/outputs/expenditure-51+.png\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "# Question 4: Ridge and Lasso Regression (30 pts)\n",
        "\n",
        "Download the dataset **Admission.csv** from Canvas and use the following codes to import the Admission dataset in Python. \n",
        "\n",
        "There are 7 features in the dataset:\n",
        "\n",
        "1. GRE score\n",
        "2. TOEFL score\n",
        "3. University Rating\n",
        "4. SOP(Statement of Purpose)\n",
        "5. LOR(Letter of Recommendation)\n",
        "6. CGPA\n",
        "7. Research\n",
        "\n",
        "And the target is **Chance of Admission**."
      ],
      "metadata": {
        "id": "gth0D8jiMBSe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# # Only use this code block if you are using Google Colab.\n",
        "# # If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
        "# from google.colab import files\n",
        "\n",
        "# # It will prompt you to select a local file. Click on “Choose Files” then select and upload the file. \n",
        "# # Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
        "# uploaded = files.upload()"
      ],
      "outputs": [],
      "metadata": {
        "id": "QFazlpLgGpAa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Codes below will work for both Google Colab and Jupyter Notebook.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "## Load the dataset into pandas DataFrame\n",
        "df = pd.read_csv('Admission.csv', index_col=0)\n",
        "df = df.replace([np.inf, -np.inf], np.nan) # \n",
        "df = df.fillna(0) # Replace all the NaN values with 0"
      ],
      "outputs": [],
      "metadata": {
        "id": "xsPaOOehGuU6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.columns # Show you all the columns in this file"
      ],
      "outputs": [],
      "metadata": {
        "id": "PmMz72U6Gv-z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.head() # Show you the first 5 rows in this file"
      ],
      "outputs": [],
      "metadata": {
        "id": "CyZN-yFfGxlt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y = df['Chance_of_Admit'] # The column named Chance_of_Admit is used as the target, and we store it in y\n",
        "X = df.drop(['Chance_of_Admit'], axis=1) # We keep the remaining columns as the features, and store them in x"
      ],
      "outputs": [],
      "metadata": {
        "id": "S3TfR0i4G2rO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 1\n",
        "\n",
        "(**2 pts**) Split the data into a training set(75% of data) and a test set(25% of data), using the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function with random_state = 50. Then scale the data (not including target) so that each of the independent variables would have zero mean and unit variance. You can use the [sklearn.preprocessing.scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) function for this. Print the first 5 rows of the training set after scaling."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=50)\n",
        "print(\"Size of Original data: \",y.shape[0])\n",
        "print(\"Size of Test data (25%): \",y_test.shape[0])\n",
        "print(\"Size of Training data (75%): \",y_train.shape[0])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.preprocessing import scale\n",
        "X_train_tr = scale(X_train)\n",
        "X_test_tr = scale(X_test)\n",
        "X_train_tr[0:5]"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 2\n",
        "\n",
        "(**5 pts**) Use [sklearn.linear_model.Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) and [sklearn.linear_model.Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) classes to do a **5-fold** cross validation using sklearn's KFold. For the sweep of the regularization parameter, we will look at a grid of values ranging from α=10^10 to α=10^−6. In Python, you can consider this range of values as follows: alpha = 10**numpy.linspace(6,-6,100) \n",
        "so that you can generate 100 uniform values between -6 to 6 as power series.\n",
        "\n",
        "Fit the 2 regression models with scaled data and report the best chosen **α** based on cross validation as well as the corresponding scoring metric. The cross validation should happen on your training data using **MSE** as the scoring metric.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "alpha_list = 10**np.linspace(10,-6,100)\n",
        "\n",
        "# Lasso Regression\n",
        "\n",
        "cv = []\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    cv_error = []\n",
        "    for train_index, test_index in KFold(n_splits=5).split(X_train_tr):\n",
        "        lmdl = linear_model.Lasso(alpha= alpha)\n",
        "        lmdl.fit(X_train_tr[train_index],y_train.iloc[train_index])\n",
        "        lmdl_preds = lmdl.predict(X_train_tr[test_index])\n",
        "        cv_error.append(mean_squared_error(lmdl_preds, y_train.iloc[test_index]))\n",
        "    cv.append(np.mean(cv_error))\n",
        "    \n",
        "cv = pd.DataFrame(cv, index=alpha_list, columns=[\"MSE\"])\n",
        "lasso_alpha = cv[\"MSE\"].idxmin()\n",
        "lasso_error = cv.loc[lasso_alpha][\"MSE\"]\n",
        "print(\"Lasso Regression Cross-Validation:\\n\\toptimal \\u03B1 = {alpha}\\n\\tMSE: {mse}\".format(alpha = lasso_alpha, mse = lasso_error))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Ridge Regression\n",
        "\n",
        "cv = []\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    cv_error = []\n",
        "    for train_index, test_index in KFold(n_splits=5).split(X_train_tr):\n",
        "        rmdl = linear_model.Ridge(alpha= alpha)\n",
        "        rmdl.fit(X_train_tr[train_index],y_train.iloc[train_index])\n",
        "        rmdl_preds = rmdl.predict(X_train_tr[test_index])\n",
        "        cv_error.append(mean_squared_error(rmdl_preds, y_train.iloc[test_index]))\n",
        "    # cv.append(cv_error)\n",
        "    cv.append(np.mean(cv_error))\n",
        "    \n",
        "cv = pd.DataFrame(cv, index=alpha_list, columns=[\"MSE\"])\n",
        "ridge_alpha = cv[\"MSE\"].idxmin()\n",
        "ridge_error = cv.loc[ridge_alpha][\"MSE\"]\n",
        "print(\"Ridge Regression Cross-Validation:\\n\\toptimal \\u03B1 = {alpha}\\n\\tMSE: {mse}\".format(alpha = ridge_alpha, mse = ridge_error))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 3\n",
        "\n",
        "(**5 pts**) Run ridge and lasso regression for all of the **α** specified above (on training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; different features' weights of each model should be on the same plot with different colors (3pts). \n",
        "\n",
        "What do you qualitatively observe when the value of the regularization parameter changes (2pts)?\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "lasso_coeffs = pd.DataFrame(columns=X_train.columns, index=alpha_list)\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    lmdl = linear_model.Lasso(alpha= alpha)\n",
        "    lmdl.fit(X_train_tr,y_train)\n",
        "    lasso_coeffs.loc[alpha] = lmdl.coef_"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "ridge_coeffs = pd.DataFrame(columns=X_train.columns, index=alpha_list)\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    rmdl = linear_model.Ridge(alpha= alpha)\n",
        "    rmdl.fit(X_train_tr,y_train)\n",
        "    ridge_coeffs.loc[alpha] = rmdl.coef_"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1,2, sharey=True, sharex=True)\n",
        "fig.set_size_inches(15, 7,  forward=True)\n",
        "\n",
        "ax[0].plot(np.log10(alpha_list),lasso_coeffs, label = lasso_coeffs.columns)\n",
        "ax[0].set_ylabel(\"Coefficient Values\")\n",
        "ax[0].set_xlabel(\"log(\\u03B1)\")\n",
        "ax[0].set_title(\"Lasso Regression\")\n",
        "ax[0].set_label(lasso_coeffs.columns)\n",
        "ax[0].grid(True)\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(np.log10(alpha_list),ridge_coeffs, label = ridge_coeffs.columns)\n",
        "# ax[1].set_ylabel(\"Coefficient Values\")\n",
        "ax[1].set_xlabel(\"log(\\u03B1)\")\n",
        "ax[1].set_title(\"Ridge Regression\")\n",
        "ax[1].grid(True)\n",
        "ax[1].legend()\n",
        "\n",
        "fig.suptitle(\"Comparasion between Lasso and Ridge coefficients\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For **LASSO REGRESSION**, as the $\\alpha$ value increases, i.e: the regularisation parameter increases, the value of the coefficients is set to **Zero**. Thus, We observe the curves dropping sharply to 0\n",
        "\n",
        "For **RIDGE REGRESSION** however, as the $\\alpha$ value  increases, the value of the coefficients gradually approach **ZERO**. this is evident by the much smoother curves in Ridge. One surprising feature in ridge regression is that the coefficient values seem to increase first (around $log_{10}(\\alpha) \\in (0,3) $) and then drops sharply to zero (around $log_{10}(\\alpha) \\in (3,6) $)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 4\n",
        "\n",
        "(**3 pts**) Take the exponential of Y_train as the target, and fit the 2 regression models again. Report the best chosen **α** based on cross validation as well as the corresponding scoring metric. Compare the results of using the original target with the results of using the exponential of the target. What do you observe? "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Lasso Regression\n",
        "\n",
        "cv = []\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    cv_error = []\n",
        "    for train_index, test_index in KFold().split(X_train_tr):\n",
        "        lmdl = linear_model.Lasso(alpha= alpha)\n",
        "        lmdl.fit(X_train_tr[train_index],np.exp(y_train.iloc[train_index]))\n",
        "        lmdl_preds = lmdl.predict(X_train_tr[test_index])\n",
        "        cv_error.append(mean_squared_error(lmdl_preds, np.exp(y_train.iloc[test_index])))\n",
        "    # cv.append(cv_error)\n",
        "    cv.append(np.mean(cv_error))\n",
        "    \n",
        "cv = pd.DataFrame(cv, index=alpha_list, columns=[\"MSE\"])\n",
        "exp_lasso_alpha = cv[\"MSE\"].idxmin()\n",
        "exp_lasso_error = cv.loc[exp_lasso_alpha][\"MSE\"]\n",
        "print(\"Lasso Regression Cross-Validation:\\n\\toptimal \\u03B1 = {alpha}\\n\\tMSE: {mse}\".format(alpha = exp_lasso_alpha, mse = exp_lasso_error))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Ridge Regression\n",
        "\n",
        "cv = []\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    cv_error = []\n",
        "    for train_index, test_index in KFold().split(X_train_tr):\n",
        "        rmdl = linear_model.Ridge(alpha= alpha)\n",
        "        rmdl.fit(X_train_tr[train_index],np.exp(y_train.iloc[train_index]))\n",
        "        rmdl_preds = rmdl.predict(X_train_tr[test_index])\n",
        "        cv_error.append(mean_squared_error(rmdl_preds, np.exp(y_train.iloc[test_index])))\n",
        "    # cv.append(cv_error)\n",
        "    cv.append(np.mean(cv_error))\n",
        "    \n",
        "cv = pd.DataFrame(cv, index=alpha_list, columns=[\"MSE\"])\n",
        "exp_ridge_alpha = cv[\"MSE\"].idxmin()\n",
        "exp_ridge_error = cv.loc[exp_ridge_alpha][\"MSE\"]\n",
        "print(\"Ridge Regression Cross-Validation:\\n\\toptimal \\u03B1 = {alpha}\\n\\tMSE: {mse}\".format(alpha = exp_ridge_alpha, mse = exp_ridge_error))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon training our models against $e^{y\\_train}$ the MSE of the optimal model increases. This is because the $\\sigma_{e^{y\\_train}}$ _(variance)_ and the $\\mu_{e^{y\\_train}}$ _(mean)_ also increases when y_train is exponentiated.\n",
        "\n",
        "The Optimal $\\alpha$ for both Lasso and Ridge Regression increases too.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 5\n",
        "\n",
        "(**5 pts**) Similarly, use [sklearn.linear_model.ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) to do linear regression with different **α** values, and plot the coefficients learned for each of them (2pts). Observe the plot, then explain the pros and cons of ridge, lasso and Elastic Net models (3pts)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "elastic_coeffs = pd.DataFrame(columns=X_train.columns, index=alpha_list)\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    enet_mdl = linear_model.ElasticNet(alpha= alpha)\n",
        "    enet_mdl.fit(X_train_tr,y_train)\n",
        "    elastic_coeffs.loc[alpha] = enet_mdl.coef_\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(7, 7,  forward=True)\n",
        "\n",
        "ax.plot(np.log10(alpha_list),elastic_coeffs, label = elastic_coeffs.columns)\n",
        "ax.set_ylabel(\"Coefficient Values\")\n",
        "ax.set_xlabel(\"log(\\u03B1)\")\n",
        "ax.set_title(\"ElasticNet Regression\")\n",
        "ax.grid(True)\n",
        "ax.legend()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso Regression\n",
        "\n",
        "- Acronym for *Least Absolute Shrinkage Selector Operator*\n",
        "- Lasso assigns a penalty _(Loss Function)_ equal to the MSE + the absolute sum of all coefficients:\n",
        "$$L = \\sum_i(\\hat{Y_i} - Y_i)^2 + \\alpha\\sum_i|\\beta_i|$$\n",
        "\n",
        "### **Pros**\n",
        "\n",
        "- Assigns ZERO weight to coefficients, thereby reducing model complexity and preventing model overfitting\n",
        "- Faster Convergence\n",
        "\n",
        "### **Cons**\n",
        "\n",
        "- Increases Bias in a model\n",
        "- Cannot deal with Multicolineraity _(or, correlated dependent variables)_\n",
        "\n",
        "## Ridge Regression\n",
        "\n",
        "- Similar to Lasso, but assigns squared magnitude of the coefficients to the loss function\n",
        "$$L = \\sum_i(\\hat{Y_i} - Y_i)^2 + \\alpha\\sum_i(\\beta_i)^2$$\n",
        "\n",
        "### **Pros**\n",
        "\n",
        "- More Versatile than Lasso\n",
        "- Does not randomly discard variables by setting them to Zero.\n",
        "\n",
        "### **Cons**\n",
        "\n",
        "- Models more difficult to interpret\n",
        "- Does not set feature coefficients to Zero, thereby retaining insignificant features and increasing complexity and variance of the model.\n",
        "\n",
        "## ElasticNet\n",
        "\n",
        "- Middle ground between Lasso and ridge regression.\n",
        "- Loss function includes both the absolute sum and squared sum of coefficients.\n",
        "$$L = \\sum_i(\\hat{Y_i} - Y_i)^2 + \\alpha\\sum_i|\\beta_i| + \\alpha\\sum_i(\\beta_i)^2$$\n",
        "\n",
        "### **Pros**\n",
        "\n",
        "- Faster Convergence than Ridge Regression\n",
        "- Prevents the model from overfitting by eliminating irrelevant features\n",
        "- Reduces impact of features not affecting the target variable (instead of stting them to 0)\n",
        "\n",
        "### **Cons**\n",
        "\n",
        "- Loss Function more complex\n",
        "- Computationally intensive as compared to Lasso and Ridge Regression\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 6\n",
        "\n",
        "(**5 pts**) Run the following three regression models with **MSE** loss on the training data: \n",
        "\n",
        "a. linear regression without regularization (1pts)\n",
        "\n",
        "b. linear regression with ridge regularization (2pts)\n",
        "\n",
        "c. linear regression with lasso regularization (2pts)\n",
        "\n",
        "For part (b) and (c), use only the best regularization parameters. Report the MSE and R<sup>2</sup> on the test data for each model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Linear Regression without Regularisation\n",
        "lm = linear_model.LinearRegression()\n",
        "lm.fit(X_train_tr,y_train)\n",
        "pred = lm.predict(X_test_tr)\n",
        "lm_mse = mean_squared_error(pred, y_test)\n",
        "lm_r2 = r2_score(y_test, pred)\n",
        "\n",
        "print(\"Metrics - Linear Regression (No Regularisation):\")\n",
        "print(\"\\tMSE:\", lm_mse)\n",
        "print(\"\\tR\\u00b2:\", lm_r2)\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_mdl = linear_model.Lasso(alpha=lasso_alpha)\n",
        "lasso_mdl.fit(X_train_tr,y_train)\n",
        "pred = lasso_mdl.predict(X_test_tr)\n",
        "lasso_mse = mean_squared_error(pred, y_test)\n",
        "lasso_r2 = r2_score(y_test, pred)\n",
        "\n",
        "\n",
        "print(\"\\nMetrics - Lasso Regression (\\u03B1 = {alp}):\".format(alp = lasso_alpha.round(5)))\n",
        "print(\"\\tMSE:\", lasso_mse)\n",
        "print(\"\\tR\\u00b2:\", lasso_r2)\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_mdl = linear_model.Ridge(alpha=ridge_alpha)\n",
        "ridge_mdl.fit(X_train_tr,y_train)\n",
        "pred = ridge_mdl.predict(X_test_tr)\n",
        "ridge_mse = mean_squared_error(pred, y_test)\n",
        "ridge_r2 = r2_score(y_test, pred)\n",
        "\n",
        "\n",
        "print(\"\\nMetrics - Ridge Regression (\\u03B1 = {alp}):\".format(alp = ridge_alpha.round(2)))\n",
        "print(\"\\tMSE:\", ridge_mse)\n",
        "print(\"\\tR\\u00b2:\", ridge_r2)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 7\n",
        "\n",
        "(**5 pts**) Train the 3 models and report the metrics with the original data without scaling (3pts). \n",
        "\n",
        "Why do we need to scale the data before regularization (2pts)? \n",
        "\n",
        "## Answer:\n"
      ],
      "metadata": {
        "id": "pKq1KERxJw9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Linear Regression without Regularisation\n",
        "lm = linear_model.LinearRegression()\n",
        "lm.fit(X_train,y_train)\n",
        "pred = lm.predict(X_test)\n",
        "lm_mse = mean_squared_error(pred, y_test)\n",
        "lm_r2 = r2_score(y_test, pred)\n",
        "\n",
        "print(\"Metrics - Linear Regression (No Regularisation):\")\n",
        "print(\"\\tMSE:\", lm_mse)\n",
        "print(\"\\tR\\u00b2:\", lm_r2)\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_mdl = linear_model.Lasso(alpha=lasso_alpha)\n",
        "lasso_mdl.fit(X_train,y_train)\n",
        "pred = lasso_mdl.predict(X_test)\n",
        "lasso_mse = mean_squared_error(pred, y_test)\n",
        "lasso_r2 = r2_score(y_test, pred)\n",
        "\n",
        "\n",
        "print(\"\\nMetrics - Lasso Regression (\\u03B1 = {alp}):\".format(alp = lasso_alpha.round(5)))\n",
        "print(\"\\tMSE:\", lasso_mse)\n",
        "print(\"\\tR\\u00b2:\", lasso_r2)\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_mdl = linear_model.Ridge(alpha=ridge_alpha)\n",
        "ridge_mdl.fit(X_train,y_train)\n",
        "pred = ridge_mdl.predict(X_test)\n",
        "ridge_mse = mean_squared_error(pred, y_test)\n",
        "ridge_r2 = r2_score(y_test, pred)\n",
        "\n",
        "\n",
        "print(\"\\nMetrics - Ridge Regression (\\u03B1 = {alp}):\".format(alp = ridge_alpha.round(2)))\n",
        "print(\"\\tMSE:\", ridge_mse)\n",
        "print(\"\\tR\\u00b2:\", ridge_r2)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why do we need to scale the data?\n",
        "\n",
        "The primary objective of scaling variables before regularisation is to ensure that the $\\alpha$ term regularises/affects each term/column/feature equally. \n",
        "\n",
        "For ex. Suppose we have the weights(in grams) and the age of individuals. The age will be in the order of $10^1$ while the weight will be in the order of $10^4$. Applying OLS with regularisation on this dataset will skew the model coefficients towards either of the features. The output of this dataset would be irrelevant because it would not take into account the true relationship between weights and age (owing to the huge disparity in the range and order).\n",
        "\n",
        "Scaling the data however, will normalise the values (say to a standard normal). Now both weight and age will range from -1 to 1 and will be centered around 0 (if standard normal scaling was implemented). This scaled dataset will now allow the regularisation term $(\\alpha)$ to \"push-down\" coefficient values more evenly, thereby allowing Regression algorithm to work efficiently and converge to the optimal solution.\n",
        "\n",
        "***\n",
        "***\n"
      ],
      "metadata": {}
    }
  ]
}