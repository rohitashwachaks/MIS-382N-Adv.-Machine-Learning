{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjzT9K-gmtJS"
      },
      "source": [
        "# <p style=\"text-align: center;\">MIS 382N: Advanced Machine Learning</p>\n",
        "# <p style=\"text-align: center;\">Homework 5</p> (Finally the final homework this semester, yay!)\n",
        "## <p style=\"text-align: center;\">Total points: 55 </p>\n",
        "## <p style=\"text-align: center;\">Due: Monday, **Nov 29th** submitted via Canvas by 11:59 pm</p>\n",
        "\n",
        "Your homework should be written in a **Jupyter notebook**. Please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting (%matplotlib inline). \n",
        "\n",
        "**Note: Notebooks MUST have the images embedded in them. There will be no regrades if attached images do not render in the notebook. Please re download from canvas after submission and make sure all attached images render without errors. (Hint: Image module from IPython.display)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfPMQCbGnKaW"
      },
      "source": [
        "**This can be an individual assignment or group of 2. If you choose to do it as a group, please specify who you are working with (name and EID), then only one student should submit the homework. Put your name and eid here.**\n",
        "\n",
        "Name: **Rohitashwa Chakraborty**\n",
        "\n",
        "EID: **rc47878**\n",
        "\n",
        "Name: **Sahitya Sundar Raj Vijayanagar**\n",
        "\n",
        "EID: **sv25849**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld3ZWXbOoTE8"
      },
      "source": [
        "# Question 1: Ensembles Conceptual (5 pts)\n",
        "Briefly describe the concepts of Gradient Boosting in your own words. How does it differ from Adaboost?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c8qvF0Bq33L"
      },
      "source": [
        "## Answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bleh\n",
        "\n",
        "### AdaBoost:\n",
        "\n",
        "- Lots of weak learns created. These weak learners are _\"stumps\"_ i.e: they just have one parent node and 2 children (depth = 1).\n",
        "- Unlike random forest where the final result is an average of all the individual trees in the forest, the stumps of Ada-boost have different weights associated with them. The net result is a _Weighted Average_ of all the individual trees/stumps \n",
        "- Each successive tree/stump is created on top of the previous stump (which is unlike random forest, where each trees grows independently).\n",
        "- Weights are directly proportional to the log-odds of classification or the gini-index of the split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ftHMxZX1fnh"
      },
      "source": [
        "# Question 2: SVM (25 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification and regression. In this problem, you will be playing with SVM on the sklearn wine dataset to explore the impacts of different parameters.\n",
        "\n",
        "**a) (5 pts)** Implement the `train_model()` function. This function takes as input:\n",
        "\n",
        "- X (the features)\n",
        "- y (the labels)\n",
        "- kernel (the specified kernel type, default value is `'linear'`)\n",
        "- C (the penalty parameter, default value is 1\n",
        ")\n",
        "- gamma (the kernel coefficient, default value is 0.5). \n",
        "\n",
        "The `train_model()` function should fit a [svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model and return the trained model. After implementing `train_model()`, use the `plot_model()` function provided below to plot the results of your model.\n",
        "\n",
        "**b) (10 pts)** The `kernel` parameter decides what form the input data will be transformed into, and may affect how your trained SVM model performs. In (a), the default value for `kernel` is `'linear'`, now re-train your svm model as you did in (a), but this time, use `'rbf'` as the `kernel`, while keeping `C=1, gamma=0.5` still. Use the provided `plot_model()` function to plot the results of your model. What do you observe?\n",
        "\n",
        "**c) (10 pts)** The `'gamma'` is a hyper-parameter needed for `'rbf'` kernel, which specifies the width of the Gaussian Kernel. Now experiment with different gamma values `[0.5, 1, 10, 100]`, use `'rbf'` as the `kernel`, while keeping `C=1`. Train your SVM and use the provided `plot_model()` function to plot the results of your model. What do you observe from the plot as gamma increases? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJxDghiN41EH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load data\n",
        "wine = datasets.load_wine()\n",
        "# only take the first two features\n",
        "X = wine.data[:, :2]\n",
        "y = wine.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ABf3DGRQ806"
      },
      "outputs": [],
      "source": [
        "def plot_model(X, y, svc, kernel='linear'):\n",
        "# You don't need to implement this function, this has been implemented and is just for plotting the trained model.\n",
        "\n",
        "# Input: \n",
        "#  - X: data features \n",
        "#  - y: the labels\n",
        "#  - svc: the trained svm.SVC model\n",
        "#  - kernel: specified kernel type, default value is 'linear'\n",
        "\n",
        "  x0_min, x0_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "  x1_min, x1_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "  h = (x0_max / x0_min)/100\n",
        "  xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, h), np.arange(x1_min, x1_max, h))\n",
        "\n",
        "  plt.subplot(1, 1, 1)\n",
        "  y_pred = svc.predict(np.c_[xx0.ravel(), xx1.ravel()])\n",
        "  y_pred = y_pred.reshape(xx0.shape)\n",
        "  plt.contourf(xx0, xx1, y_pred, cmap=plt.cm.Paired, alpha=0.2)\n",
        "\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
        "  plt.xlabel('Alcohol')\n",
        "  plt.ylabel('Malic Acid')\n",
        "  plt.xlim(xx0.min(), xx0.max())\n",
        "  plt.title('SVC with {} kernel'.format(kernel))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYMiRB-C4-Cf"
      },
      "outputs": [],
      "source": [
        "def train_model(X, y, kernel='linear', C=1, gamma=0.5):\n",
        "  svc = # IMPLEMENT ME!\n",
        "  return svc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0gqCnvO5IJj"
      },
      "outputs": [],
      "source": [
        "svc = train_model(X, y, kernel='linear', C=1, gamma=0.5)\n",
        "plot_model(X, y, svc, kernel='linear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZX8qNFpDq0Z"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYLh0dv-P6nn"
      },
      "source": [
        "# Question 3: Ensemble methods for classification (25 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this question, we will compare the performances of different ensemble methods for classification: Bagging, AdaBoost, GradientBoosting. \n",
        "\n",
        "The dataset used is [Spam Classification Data](https://archive.ics.uci.edu/ml/datasets/Spambase), which you can load from `spam_uci.csv` file. The last column represents the target label, where 1 means spam and 0 otherwise. You can use the provided codes to load the data and split training/test sets.\n",
        "\n",
        "**a) (5 pts)** Fit a [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) with `random_state=10`,  and a [Logistic Regression Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) with `random_state=10` and `solver=\"newton-cg\"` for the spam classification problem. For each classifier, report the [accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) on the test data. \n",
        "**Note**: Before computing roc_auc_score, you will need [`predict_proba(X_test)[:, 1]`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba) to obtain the predicted target scores first.\n",
        "\n",
        "\n",
        "**b) (5 pts)** For **each** classifier in (a), use [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) to create an ensemble of 50 classifiers (i.e `n_estimators=50`) with `random_state=10`, report the accuracy_score and roc_auc_score on the test data. Compare the scores with what you obtained in (a), briefly describe the impact of Bagging on both classifiers.\n",
        "\n",
        "\n",
        "**c) (5 pts)** Fit a [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for the spam classification problem. Use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to find the best combination of hyperparameters from `{\"n_estimators\": [10, 100, 500], \"criterion\": ['gini', 'entropy'], \"random_state\": [42]}`. As what you did in (a), report the accuracy_score and roc_auc_score on the test data. \n",
        "\n",
        "\n",
        "**d) (6 pts)** Fit a [GradientBoosting Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), and an [AdaBoost Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) for the spam classification problem. Use GridSearchCV to find the best combination of hyperparameters for each classifier: \n",
        "- For GradientBoosting, find the best combination from `{\"n_estimators\":[10, 100, 500], \"learning_rate\":[0.01, 0.1, 0.5], \"max_depth\":[3, 5, 10], \"subsample\":[0.5, 0.7, 1.0], \"random_state\":[42]}`;\n",
        "- For AdaBoost, find the best combination from `{\"n_estimators\":[10, 100, 500], \"learning_rate\":[0.01, 0.1, 0.5], \"random_state\":[42]}`. \n",
        "\n",
        "Report the accuracy_score and roc_auc_score on the test data for each classifier. The GridSearchCV may take some time, especially for GradientBoostingClassifier.\n",
        "\n",
        "**e) (4 pts)** In (c) and (d), you have obtained the best combination of hyperparameters respectively for Random Forest Classifier, Gradient Boosting Classifier and AdaBoost Classifier. \n",
        "\n",
        "Please use the best hyperparameters to initialize your classifiers, train your model, compute the accuracy_score and roc_auc_score on the test data, and plot the `accuracy_scores` of the three classifiers vs `n_estimators= [10, 100, 500]` in one plot, and plot the `roc_auc_scores` of the three classifiers vs `n_estimators= [10, 100, 500]` in another plot. That is, keep all other hyperparameters (except for `n_estimators`) as the best hyperparameters you obtained in (c) and (d), plot how the accuracy_score and roc_auc_score changes as you change the number of estimators (`n_estimators`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mysVytBbQBKj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "data = pd.read_csv('spam_uci.csv', index_col=0)\n",
        "print(data.shape)\n",
        "\n",
        "X = data.iloc[:, :56]\n",
        "y = data.iloc[:, 57]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2UqIn87QBsK"
      },
      "source": [
        "## Answer:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MIS382N-HW5 Questions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
