{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R_k_q-xlwE7"
      },
      "source": [
        "# <p style=\"text-align: center;\">MIS 382N: Advanced Machine Learning</p>\n",
        "# <p style=\"text-align: center;\">Homework 4</p>\n",
        "### <p style=\"text-align: center;\">Total points: 55 </p>\n",
        "## <p style=\"text-align: center;\">Due: Wed, **Nov, 3rd** submitted via Canvas by 11:59 pm</p>\n",
        "\n",
        "Your homework should be written in a **Jupyter notebook**. Please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting (%matplotlib inline). \n",
        "\n",
        "**Note: Notebooks MUST have the images embedded in them. There will be no regrades if attached images do not render in the notebook. Please re download from canvas after submission and make sure all attached images render without errors. (Hint: Image module from IPython.display)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62r4Kaoly_t"
      },
      "source": [
        "**This can be an individual assignment or group of 2. If you choose to do it as a group, please specify who you are working with (name and EID), then only one student should submit the homework. Put your name and eid here.**\n",
        "\n",
        "Name:\n",
        "\n",
        "EID:\n",
        "\n",
        "Name:\n",
        "\n",
        "EID:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJFRr3e-J9lr"
      },
      "source": [
        "# Question 1: Reject option (10 pts)\n",
        "\n",
        "Consider a binary classification problem with the following loss matrix, where the cost of rejection is a constant. \n",
        "\n",
        "$$\n",
        "   {\\begin{array}{ccccc}\n",
        "   & & \\text{Predicted class} & \\text{           } &\\\\\n",
        "   & & C_1 & C_2 & Reject\\\\\n",
        "   \\text{True class} & C_1 & 0 & 2 & c  \\\\\n",
        "   & C_2 & 4 & 0 & c \\\\\n",
        "  \\end{array} } \n",
        "$$\n",
        "\n",
        "If $c$ = 1, determine the respective predicted classes that will minimize the expected loss when $P(C_1|x)$ falls in different value intervals. That is, please give the decision rules for $C_1$, $C_2$ and $Reject$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3dMm-ADQ9sr"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXZOdsIClzeq"
      },
      "source": [
        "# Question 2: Logistic Regression (15 pts) \n",
        "\n",
        "Logistic regression has been traditionally used to classify data when a **linear decision boundary** is adequate. However, having an idea of what the distribution looks like could help one come up with a smart data transformation trick that could make the optimal decision boundary become (near) linear in the transformed space. In this question, you are given 2-dimensional data, and each data point belongs to either class 0 or class 1. \n",
        "\n",
        "a) **(5 pts)** Train a logistic regression classifier using X_train, y_train. Use the trained model to make predictions on X_train, X_test respectively, and then report the accuracies and F1-scores. Sklearn has a very nice [utility](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) that helps compute the precision, recall, accuracy and F1-scores. \n",
        "\n",
        "b) **(5 pts)** Plot the X_train data using matplotlib, use different colors to represent different classes. Comment on the pattern of data distributions, and reason why the logistic regression classifier in (a) does not perform very well.  \n",
        "\n",
        "c) **(5 pts)** Transform the input X_train and X_test data in such a way that a linear decision boundary will be effective. Train a new logistic regression classifier on this transformed X_train data. Use the trained model to make predictions on transformed X_train, transformed X_test respectively, and then report the accuracies and F1-scores. \n",
        "\n",
        "**Hint**: Carefully analyze the plot you made in (b), observe the decision boundaries, which should guide you towards the right direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1T7hgXEnADb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x1 = np.random.uniform(low=-7.5, high=7.5, size=300)\n",
        "x2 = np.random.uniform(low=-7.5, high=7.5, size=300)\n",
        "\n",
        "y_fn = (0.3*x1**2 + 0.8*x2**2) <10\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "X_train = np.column_stack([x1, x2])\n",
        "y_train = y_fn\n",
        "print(\"Train: \", X_train.shape)\n",
        "\n",
        "x1 = np.random.uniform(low=-7.5, high=7.5, size=100)\n",
        "x2 = np.random.uniform(low=-7.5, high=7.5, size=100)\n",
        "y_fn = (0.3*x1**2 + 0.8*x2**2) <10\n",
        "\n",
        "X_test = np.column_stack([x1, x2])\n",
        "y_test = y_fn\n",
        "print(\"Test: \", X_test.shape)\n",
        "\n",
        "X_train = X_train + np.random.random(size=X_train.shape)/10\n",
        "X_test = X_test + np.random.random(size=X_test.shape)/10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pcj3TgwnCpr"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kuJ3AByMTpI"
      },
      "source": [
        "# Question 3: Bayesian Network (5 pts)\n",
        "\n",
        "A Bayesian network is a directed acyclic graph (DAG) that captures a subset of the independence relationships of a given joint probability distribution. \n",
        "\n",
        "In a Bayesian network $G=(V,E)$, each node $i \\in V$in the directed graph corresponds to a random variable and each directed edge $e \\in E$ represents a statistical dependence. Each node is associated with a conditional probability distribution of the corresponding random variables given its parents in the DAG. The joint probability distribution factorizes w.r.t the directed graph $G$ if $p(x_1, ..., x_n)=\\prod_{i \\in V} p(x_i | x_{parents(i)})$, provided the variables are visited in a topologically sorted order. Please write the joint probability distribution that factorizes w.r.t this graph, in as simple a form as possible (i.e. that results in the smallest possible conditional probability tables:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "sfd86n0uP0dp",
        "outputId": "6cad03b4-86b3-4912-84df-dc378b00cbbb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe src=\"https://drive.google.com/file/d/1XigbvsWpNvlols3QarVjDfYf2Dnf3emy/preview\" width=\"640\" height=\"480\" allow=\"autoplay\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<iframe src=\"https://drive.google.com/file/d/1XigbvsWpNvlols3QarVjDfYf2Dnf3emy/preview\" width=\"640\" height=\"480\" allow=\"autoplay\"></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kUqyveiP6zV"
      },
      "source": [
        "## Answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPukziKknW3K"
      },
      "source": [
        "# Question 4: Counterfactuals (25 pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tASB8c_MJ2ZD"
      },
      "source": [
        "To show the effectiveness of a newly proposed model, one needs to compare it with multiple  existing solutions, across multiple datasets with different properties, and typically using multiple evaluation metrics. \n",
        "\n",
        "One of the emerging requirements in industry is to be able to explain decisions, and counterfactual (CF) explanations turn out to be a lead contender. Hence a new CF approach also needs to be evaluated comprehensively. See [1] for a review on conterfactual explanations. \n",
        "\n",
        "An example of a short comparative testing for CFs is given in [2], (you donâ€™t need to understand the method in [2] since I have not covered this type of techinques in class, just see the result tables). The goal of this question is to **try to obtain results that are similar to those reported in [2]** (note that they give details of their model settings in the appendix). In particular, you will evaluate the following datasets: Breast Cancer, Pima diabetes and Adult Census across 5 metrics (Validity, sparsity, proximity, diversity, in-distributionness). \n",
        "\n",
        "**Note**: **Section A** students only need to report on the **Breat Cancer and Adult Census datasets** while **Section B** students only need to report results on the **Pima diabetes and Adult Census datasets**.\n",
        "\n",
        "[1] Verma, Dickerson, Hines. Counterfactual Explanations for Machine Learning: A Review. [arXiv:2010.10596](https://arxiv.org/abs/2010.10596) (2020) \n",
        "\n",
        "[2] Samoilescu, et al. Model-agnostic and Scalable Counterfactual Explanations via Reinforcement Learning.  [arXiv:2106.02597](https://arxiv.org/abs/2106.02597) (2021) \n",
        "\n",
        "\n",
        "a) Train **Random Forest, MLP and Logistic Regression** models on your datasets and report the accuracies on each of the datasets, show the accuracy on both the training and test set in a table.\n",
        "\n",
        "Use the code snippets given below to load the datasets and make the split using the following code snippet: ```X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)``` \n",
        "\n",
        "```\n",
        "# Breast cancer dataset, for Section A only\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "cols = ['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
        "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
        "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
        "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
        "        'smoothness error', 'compactness error', 'concavity error',\n",
        "        'concave points error', 'symmetry error',\n",
        "        'fractal dimension error', 'worst radius', 'worst texture',\n",
        "        'worst perimeter', 'worst area', 'worst smoothness',\n",
        "        'worst compactness', 'worst concavity', 'worst concave points',\n",
        "        'worst symmetry', 'worst fractal dimension']\n",
        "X = pd.DataFrame(data=X, columns=cols)\n",
        "\n",
        "# Pima Diabetes dataset, for Section B only\n",
        "def load_diabetes():\n",
        "  df = pd.read_csv('diabetes.csv')\n",
        "  X = df.drop(['Outcome'], axis=1)\n",
        "  y = df[['Outcome']]\n",
        "  return X, np.reshape(y, (-1, ))\n",
        "\n",
        "X, y = load_diabetes()\n",
        "\n",
        "# adult census dataset, for Section A and Section B\n",
        "def load_adult():\n",
        "  df = pd.read_csv('adult.csv')\n",
        "  df = pd.get_dummies(df)\n",
        "  X = df.drop(['target'], axis=1)\n",
        "  y = df[['target']]\n",
        "  return X, np.reshape(y, (-1, ))\n",
        "\n",
        "X, y = load_adult()\n",
        "```\n",
        "b) For each of the datasets, plot the calibration curves for the test set. The [module](https://scikit-learn.org/stable/modules/calibration.html) from sklearn should help.\n",
        "\n",
        "For each dataset, the calibration curve should show a y=x line as the perfect calibration, as well as the calibration curves for each of the model predictions on the test set. Mark the plots correctly using a legend.\n",
        "\n",
        "c) [DiCE](https://github.com/interpretml/DiCE) is an open-source library that is based on the paper [Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations](https://arxiv.org/abs/1905.07697). The codebase has 3 methods in which they generate counterfactuals - random sampling, KD-tree and genetic algorithm. Use DiCE (random) and DiCE (genetic) to generate 5 counterfactuals each for the first 50 data point in the test set for each of the models. Report the following metrics for each of the datasets and each of the models. So essentially, you will need to create a table for each dataset where the metrics are present in the columns and each row represents a different model. \n",
        "\n",
        "Here are some pointers that will help you get started with DiCE and the metrics needed. \n",
        "- Install the library using ```!pip install dice-ml```\n",
        "- They have an excellent documentation on their GitHub, and their implementation notebooks will be very helpful. You can check [this](https://github.com/interpretml/DiCE/blob/master/docs/source/notebooks/DiCE_model_agnostic_CFs.ipynb) to get started.\n",
        "- Section 4.1 in the [DiCE paper](https://arxiv.org/pdf/1905.07697.pdf) discuss and mathematically formulate validity, sparsity, diversity and proximity. Read those carefully and implement functions that can evaluate the metrics. Use the ```Continuous-Proximity``` metric just for simplicity. Use the ```Diversity``` metric and not ```Count-Diversity```.\n",
        "- For the in-distributionness, use the following code snippet to evaluate the MMD (maximum mean discrepancy) score.\n",
        "\n",
        "```\n",
        "def get_mmd_rbf(X, Y, gamma=1.0):\n",
        "    \"\"\"MMD using rbf (gaussian) kernel (i.e., k(x,y) = exp(-gamma * ||x-y||^2 / 2))\n",
        "    Arguments:\n",
        "        X {[n_sample1, dim]} -- [X matrix]\n",
        "        Y {[n_sample2, dim]} -- [Y matrix]\n",
        "    Keyword Arguments:\n",
        "        gamma {float} -- [kernel parameter] (default: {1.0})\n",
        "    Returns:\n",
        "        [scalar] -- [MMD value]\n",
        "    \"\"\"\n",
        "    XX = metrics.pairwise.rbf_kernel(X, X, gamma)\n",
        "    YY = metrics.pairwise.rbf_kernel(Y, Y, gamma)\n",
        "    XY = metrics.pairwise.rbf_kernel(X, Y, gamma)\n",
        "    return XX.mean() + YY.mean() - 2 * XY.mean()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcLazqf-DLkq"
      },
      "source": [
        "## Answer:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MIS382N-HW4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
