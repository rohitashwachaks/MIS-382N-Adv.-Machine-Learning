{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIS382N-HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcxOqnDFR8Ly"
      },
      "source": [
        "# <p style=\"text-align: center;\">MIS 382N: Advanced Machine Learning</p>\n",
        "# <p style=\"text-align: center;\">Homework 2</p>\n",
        "## <p style=\"text-align: center;\">Total points: **60**</p>\n",
        "## <p style=\"text-align: center;\">Due: Monday, **Sep 27** submitted via Canvas by 11:59 pm</p>\n",
        "\n",
        "Your homework should be written in a **Jupyter notebook**. Please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting (%matplotlib inline). \n",
        "\n",
        "\n",
        "**Note: Notebooks MUST have the images embedded in them. There will be no regrades if attached images do not render in the notebook. Please make sure all attached images render without errors. (Hint: Image module from IPython.display)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZU0QWkzSDnx"
      },
      "source": [
        "**This can be an individual assignment or group of 2. If you choose to do it as a group, please specify who you are working with (name and EID), then only one student should submit the homework. Put your name and eid here.**\n",
        "\n",
        "Name:\n",
        "\n",
        "EID:\n",
        "\n",
        "Name:\n",
        "\n",
        "EID:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02CfYifUSHzJ"
      },
      "source": [
        "# Question 1: Bias-Variance (5 pts)\n",
        "\n",
        "a) (**2 pts**) Assume there is a data generator $Y=f(X)+\\epsilon$, which is generating Data(X, Y), where $\\epsilon$ is the added random gaussian noise. We are trying to fit a curve to the samples generated from the data generator, using an estimator. The estimator can be represented as $g(X|\\theta)$, where $\\theta$ represents the parameters. For any test point $x_0$, what does the following mathematical representation mean? Is this the bias or variance of the estimator? $$E[g(x_0)]-f(x_0)$$\n",
        "\n",
        "b) (**3 pts**) Use your own words to describe why there is a tradeoff between bias and variance. \n",
        "\n",
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JX0hdXUSJJj"
      },
      "source": [
        "# Question 2: Bias-Variance exploration (20 pts)\n",
        "\n",
        "We want to build a model that can predict y for unknown inputs x.\n",
        "\n",
        "(a) (**10 pts**) Fit polynomial models of degrees 2, 4, 7 to the training data. Print out the mean squared error (on both train and test sets) for all the models. Plot the data (y_train vs x_train and y_test vs x_test), the fitted models (predictions on x_all by different models vs x_all), and the predictions on the test set (predictions on x_test by different models vs x_test). All the plots must be in the same figure and be clearly labeled. **Tips**: you can use `np.vander(np.squeeze(x_train), deg+1)` to generate the `deg`-degree polynomial vector of `x_train`. For example, `np.vander(np.squeeze(x_train), 3)` gives you the second-degree polynomial of `x_train`.\n",
        "\n",
        "\n",
        "(b) (**5 pts**) Which model gives the best performance? Explain in terms of the bias-variance tradeoff.\n",
        "\n",
        "(c) (**5 pts**) Analyse how the training data size affects the bias and the variance of the models. For this, run the analysis in (a) using 20, 40, 60, 80 and all 100 data points. Make a **single** plot of the $log(MSE)$ for both the training and the test data vs the size of the training set for each of the polynomials. State the trends you see as you change the size of the training data on each of the models and explain why you see them.\n",
        "**You can use the following codes to load the dataset and complete the analysis**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CEX0XdwSo0A"
      },
      "source": [
        "from sklearn import linear_model as lm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "data_load = np.load('./data.npy', allow_pickle=True)\n",
        "x_train = data_load.item().get(\"Xtrain\")\n",
        "y_train = data_load.item().get(\"Ytrain\")\n",
        "x_test = data_load.item().get(\"Xtest\")\n",
        "y_test =data_load.item().get(\"Ytest\")\n",
        "x_all = np.linspace(-10,10,101).reshape(-1,1)\n",
        "\n",
        "lrp = LinearRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsH23wBM-pQR"
      },
      "source": [
        "def plot_curves(x_train, y_train, x_test, y_test):\n",
        "  # Fit polynomial models of degrees 2, 4, 7 to the training data. \n",
        "  # Print out the mean squared error (on both train and test sets) for all the models. \n",
        "  # Plot the data (y_train vs x_train and y_test vs x_test), the fitted models (predictions on x_all by different models vs x_all), and the predictions on the test set (predictions on x_test by different models vs x_test). \n",
        "\n",
        "  # YOUR CODE COMES HERE\n",
        "  print(\"IMPLEMENT ME!\")\n",
        "  return [0, 0, 0], [0, 0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7AHxOOE-xrH"
      },
      "source": [
        "# Fit the different polynomials to the training data and make the plots \n",
        "train_rmses_100, test_rmses_100 = plot_curves(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_Hmz7e8-0cm"
      },
      "source": [
        "# Study the effects of the training data size on the bias and variance\n",
        "print(\"20% data\")\n",
        "train_rmses_20, test_rmses_20 = plot_curves(x_train[40:60], y_train[40:60], x_test, y_test)\n",
        "print(\"40% data\")\n",
        "train_rmses_40, test_rmses_40 = plot_curves(x_train[30:70], y_train[30:70], x_test, y_test)\n",
        "print(\"60% data\")\n",
        "train_rmses_60, test_rmses_60 = plot_curves(x_train[20:80], y_train[20:80], x_test, y_test)\n",
        "print(\"80% data\")\n",
        "train_rmses_80, test_rmses_80 = plot_curves(x_train[10:90], y_train[10:90], x_test, y_test)\n",
        "print(\"100% data\")\n",
        "train_rmses_100, test_rmses_100 = plot_curves(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtseLbFDTLws"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKbRSU9tSoRD"
      },
      "source": [
        "# Question 3: Gradient descent (5 pts)\n",
        "\n",
        "a) (**2 pts**) Compare gradient descent and stochastic gradient descent in terms of their key advantages and disdvantages. Limit your answer to one paragaraph.\n",
        "\n",
        "b) (**3 pts**) Read this [blog](https://www.benfrederickson.com/numerical-optimization/) on second order optimization and answer the following question: **How does Nelder-Mead method work? What can be the major problems of Nelder-Mead method?**\n",
        "\n",
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ezhiR4cSsUs"
      },
      "source": [
        "# Question 4: Stochastic gradient descent (10 pts)\n",
        "\n",
        "Use stochastic gradient descent to derive the coefficent updates for the 4 coefficients $w_0, w_1, w_2, w_3$ in this model：\n",
        "$$ y = w_0 + w_1x_1 + w_2 x_1x_2 + w_3e^{-x_1} $$ \n",
        "\n",
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ8eqNmwTWsK"
      },
      "source": [
        "# Question 5: Stochastic gradient descent coding (20 pts)\n",
        "\n",
        "Code an SGD solution in Python for this non-linear model$$ y = w_0 + w_1x_1 + w_2x_1x_2 + w_3e^{-x_1} $$  The template of the solution class is given. The init function of the class takes as input the learning rate, regularization constant and number of epochs. The fit method must take as input X, y. The predict method takes an X value (optionally, an array of values). \n",
        "\n",
        "a) (**15 pts**) Use your new gradient descent regression to predict the data given in 'SGD_samples.csv', for 15 epochs, using learning rates: [0, .0001, .001, .01, 0.1, 1, 10, 100] and regularization (ridge regression) constants: [0,10,100]. For the best 2 combinations of learning_rate and regularization for SGD, plot MSE and the $w$ parameters as a function of epoch (for 15 epochs) . \n",
        "\n",
        "b) (**5 pts**) Report the MSE of the two best combinations of learning rate and regularization constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN4rIn-CSr0i"
      },
      "source": [
        "# Only use this code block if you are using Google Colab.\n",
        "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
        "from google.colab import files\n",
        "\n",
        "## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file. \n",
        "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l676O-DJUKUM"
      },
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "class Regression:\n",
        "    \n",
        "    def __init__(self, learning_rate, regularization, n_epoch):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epoch = n_epoch\n",
        "        self.regularization = regularization\n",
        "        # initialize whichever variables you would need here\n",
        "        self.coef = np.zeros(4)\n",
        "        \n",
        "    def sgd(self, gradient):\n",
        "        self.coef # = please put your codes here to update the self.coef using SGD\n",
        "    \n",
        "    def fit(self, X, y, update_rule='sgd', plot=False):\n",
        "        mse = []\n",
        "        coefs = []\n",
        "        X = self.get_features(X)\n",
        "        for epoch in range(self.n_epoch):\n",
        "            for i in range(X.shape[0]):\n",
        "                # Compute error\n",
        "                   #please put your codes here\n",
        "\n",
        "                # Compute gradients\n",
        "                    #please put your codes here\n",
        "               \n",
        "                # Update weights\n",
        "                self.sgd(gradient)\n",
        "\n",
        "            coefs.append(self.coef)\n",
        "            residuals = y - self.linearPredict(X)         \n",
        "            mse.append(np.mean(residuals**2))\n",
        "\n",
        "        self.lowest_mse = mse[-1]\n",
        "        if plot == True:\n",
        "            plt.figure()\n",
        "            plt.plot(range(self.n_epoch),mse)\n",
        "            plt.xlabel('epoch')\n",
        "            plt.ylabel('MSE')\n",
        "            plt.figure()\n",
        "            coefs = np.array(coefs)\n",
        "            plt.plot(range(self.n_epoch),coefs[:,0],label='w0')\n",
        "            plt.plot(range(self.n_epoch),coefs[:,1],label='w1')\n",
        "            plt.plot(range(self.n_epoch),coefs[:,2],label='w2')\n",
        "            plt.plot(range(self.n_epoch),coefs[:,3],label='w3')\n",
        "            plt.legend()\n",
        "            plt.xlabel('epoch')\n",
        "            plt.ylabel('parameter value')\n",
        "\n",
        "    def get_features(self, X):\n",
        "        '''\n",
        "        this output of this function can be used to compute the gradient in `fit`\n",
        "        '''\n",
        "        x = np.zeros((X.shape[0], 4))\n",
        "        x[:,0] = 1\n",
        "        x[:,1] = X[:,0]\n",
        "        x[:,2] = X[:,0]*X[:,1]\n",
        "        x[:,3] = np.exp(-X[:,0])\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    def linearPredict(self, X):  \n",
        "      #compute the dot product of self.coef and X\n",
        "      return None #this line is just a placeholder, please delete this line in your code        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvOp8kdxUNCP"
      },
      "source": [
        "data = pd.read_csv('SGD_samples.csv')\n",
        "X = np.array([data['x1'].values, data['x2'].values]).T\n",
        "y = data['y'].values\n",
        "n_epochs = 15\n",
        "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "regularization = [0, 10, 100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na3FbmyUUQF3"
      },
      "source": [
        "## Answer:"
      ]
    }
  ]
}